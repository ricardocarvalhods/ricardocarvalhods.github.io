[{"authors":["admin"],"categories":null,"content":"I'm Ricardo Carvalho, a Computer Science PhD student at Simon Fraser University (SFU). My current research focus on Machine Learning, mostly Generative Adversarial Networks (GANs), and Differential Privacy (DP).\nBefore joining SFU, I worked as a Senior Data Scientist for the Brazilian Government for over 6 years and carried out consultancy work for companies and individuals interested in connecting data science to businesses.\nI also worked for almost a year as Data Science Manager at the Observatory of Public Spending (ODP), leading more than 15 data professionals on projects related to data science, fraud prediction and auditing.\n","date":1580256000,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1580256000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I'm Ricardo Carvalho, a Computer Science PhD student at Simon Fraser University (SFU). My current research focus on Machine Learning, mostly Generative Adversarial Networks (GANs), and Differential Privacy (DP).\nBefore joining SFU, I worked as a Senior Data Scientist for the Brazilian Government for over 6 years and carried out consultancy work for companies and individuals interested in connecting data science to businesses.\nI also worked for almost a year as Data Science Manager at the Observatory of Public Spending (ODP), leading more than 15 data professionals on projects related to data science, fraud prediction and auditing.","tags":null,"title":"Ricardo Carvalho","type":"authors"},{"authors":["Ricardo Carvalho"],"categories":["TensorFlow","Python"],"content":"Below we give a jupyter notebook containing the implementation of a Differentially Private Conditional GAN, originally described on Torkzadehmahani et al. 2019, with explanation of every step implemented.\nWe include a TensorFlow 2 version implemented from scratch, using the Keras API and a tf.GradientTape training loop. Also to successfully use DP on a Conditional GAN, we design a custom optimizer. Experiments used the MNIST dataset.\nPre-requisites: (see links on notebook for suggestions of references)\n Generative Adversarial Networks (GANs) Differential Privacy (DP), especially DP-SGD  We focus on the hypothetical scenario where:\n Training data is considered sensitive. We aim to release a \u0026ldquo;safe\u0026rdquo; version (that does not compromise privacy) of the training data to the public. The goal is to allow external people to use the \u0026ldquo;safe\u0026rdquo; training data to create a classification model that performs well on the real testing data.  Therefore, we validate our GANs by creating models on the generated data and measuring performance.    If you intend to use any GAN with DP please consider the following notes.\nImportant notes about using DP with GANs:\n Usually the Generator is trained without DP, therefore we cannot show the data labels to the Generator. We sample labels uniformly at random on each training step. Although the notebook shows the one training, to report results we should train it many times from scratch and show averaged results, not maximum or single result. Our Validation uses the training data again to evaluate the GAN. To be 100% DP, we need to either pay some privacy budget for this step, or just use the actual generated data with cross validation for evalution. When generating data for creating classification models, we create the same amount of data for each label. Using the real test distribution of labels to determine the number of examples generated for each label assumes this information is public, which is rarely the case.  Links:\n Open directly on Google Colab by clicking here  It takes 3+ hours to run the experiment completely once on Google Colab GPU   Github repository (with more info about method) Jupyter Notebook  ","date":1580256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580256000,"objectID":"a81eece1b4256f5e7708a4fc15474c5e","permalink":"/post/dpcgan/","publishdate":"2020-01-29T00:00:00Z","relpermalink":"/post/dpcgan/","section":"post","summary":"I describe a TF 2 implementation of a Differentially Private Conditional GAN, originally described on Torkzadehmahani et al. 2019.","tags":["Differential Privacy","GAN","TensorFlow"],"title":"DP-CGAN: Differentially Private Conditional GAN on TensorFlow 2 explained","type":"post"},{"authors":["Ricardo Silva Carvalho","Ke Wang","Lovedeep Gondara"],"categories":["Differential Privacy","Data Analysis"],"content":"Currently, I'm updating the latest version to address reviewersâ€™ comments and will soon release the full final paper.\nWe also have extensive code to reproduce all the results, in pure python and also notebooks to simplify understanding. So stay tuned!\n","date":1578347902,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578347902,"objectID":"ca32777f8f8081648c381488829048f4","permalink":"/publication/differentially-private-top-k-selection-for-unknown-data-domain/","publishdate":"2020-01-15T18:58:22-03:00","relpermalink":"/publication/differentially-private-top-k-selection-for-unknown-data-domain/","section":"publication","summary":"We propose new methods that satisfy approximate differential privacy for top-$k$ selection on the unknown data domain setting, not relying on knowledge of domain universe and without assuming any structure of the data. Different from classical methods, our algorithms only require looking at the top-$\\bar{k}$ elements for any $\\bar{k} \\geq k$. By not requiring knowledge of domain and limiting the elements considered, we enforce the principle of minimal privilege, which requires granting the strictly necessary access for a given task. Additionally, we also design a novel way of optimizing the choice of $\\bar{k}$ based on a dataset while maintaining privacy. We extensively compare our main algorithm to previous works in the same setting and show, both analytically and on experiments, that we outperform the state-of-the-art, significantly improving utility.","tags":["Differential Privacy","Data Analysis"],"title":"Differentially Private Top-k Selection for Unknown Data Domain","type":"publication"},{"authors":null,"categories":null,"content":"After an entire year of doing research on Differential Privacy, I had my first paper as a Ph.D. student accepted to an important conference: AISTATS.\n Title: Differentially Private Top-k Selection for Unknown Data Domain Authors: Ricardo Silva Carvalho, Ke Wang, Lovedeep Gondara Conference: AISTATS 2020, the 23rd International Conference on Artificial Intelligence and Statistics  I'm now updating the latest version to address reviewers\u0026rsquo; comments and will soon release the full final paper.\nWe also have extensive code to reproduce all the results, in python with notebooks to simplify understanding.\nStay tuned!\n","date":1578268800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578268800,"objectID":"162b4eff962882ccd2c5ebcef0f25ed5","permalink":"/news/aistats20/","publishdate":"2020-01-06T00:00:00Z","relpermalink":"/news/aistats20/","section":"news","summary":"After an entire year of doing research on Differential Privacy, I had my first paper as a Ph.D. student accepted to an important conference: AISTATS.\n Title: Differentially Private Top-k Selection for Unknown Data Domain Authors: Ricardo Silva Carvalho, Ke Wang, Lovedeep Gondara Conference: AISTATS 2020, the 23rd International Conference on Artificial Intelligence and Statistics  I'm now updating the latest version to address reviewers\u0026rsquo; comments and will soon release the full final paper.","tags":null,"title":"Paper accepted to AISTATS 2020 on differentially private top-k selection","type":"news"},{"authors":["Lovedeep Gondara","Ke Wang","Ricardo Silva Carvalho"],"categories":["Differential Privacy","Lottery Ticket","Stochastic Gradient Descent","Deep Learning"],"content":"This is a work in progress presented at the Workshop on Machine Learning with Guarantees, part of NeurIPS 2019, as a POSTER.\n","date":1576362568,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576362568,"objectID":"df8d3490192b2a3af749b8065536e4a5","permalink":"/publication/winning-privately-the-differentially-private-lottery-ticket-mechanism/","publishdate":"2019-12-14T19:29:28-03:00","relpermalink":"/publication/winning-privately-the-differentially-private-lottery-ticket-mechanism/","section":"publication","summary":"We propose the differentially private lottery ticket mechanism (DPLTM). An end-to-end differentially private training paradigm based on the lottery ticket hypothesis. Using \"high-quality winners\", selected via our custom score function, DPLTM significantly outperforms state-of-the-art. We show that DPLTM converges faster, allowing for early stopping with reduced privacy budget consumption. We further show that the tickets from DPLTM are transferable across datasets, domains, and architectures. Our extensive evaluation on several public datasets provides evidence to our claims.","tags":["Differential Privacy","Lottery Ticket","Stochastic Gradient Descent","Deep Learning"],"title":"Winning Privately: The Differentially Private Lottery Ticket Mechanism","type":"publication"},{"authors":null,"categories":null,"content":"This is a work in progress presented at the Workshop on Machine Learning with Guarantees, part of NeurIPS 2019, as a POSTER.\nMy colleague Lovedeep Gondara will present it on Saturday, Dec 14th, at the Workshop on Machine Learning with Guarantees (NeurIPS 19):\n Title: Winning Privately: The Differentially Private Lottery Ticket Mechanism Authors: Lovedeep Gondara, Ke Wang, Ricardo Silva Carvalho Conference: Workshop on Machine Learning with Guarantees, at the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019).  ","date":1576022400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576022400,"objectID":"3bd27e2f4fd8a9dadfdb5407a2b7835b","permalink":"/news/workshop19/","publishdate":"2019-12-11T00:00:00Z","relpermalink":"/news/workshop19/","section":"news","summary":"This is a work in progress presented at the Workshop on Machine Learning with Guarantees, part of NeurIPS 2019, as a POSTER.\nMy colleague Lovedeep Gondara will present it on Saturday, Dec 14th, at the Workshop on Machine Learning with Guarantees (NeurIPS 19):\n Title: Winning Privately: The Differentially Private Lottery Ticket Mechanism Authors: Lovedeep Gondara, Ke Wang, Ricardo Silva Carvalho Conference: Workshop on Machine Learning with Guarantees, at the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019).","tags":null,"title":"Poster accepted to Workshop on ML with Guarantees at NeurIPS 2019","type":"news"},{"authors":null,"categories":null,"content":"This year's NeurIPS is here at Vancouver and I'm excited to share that I have been selected as volunteer.\nI'll be at the Registration Desk on Dec 9th, Monday morning, from 7am to 12pm, and also will be monitoring doors on Dec 11th, Wednesday afternoon, from 1:45pm to 5pm.\nPlease feel free to get in touch!\n","date":1574553600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574553600,"objectID":"9e47ea341302a46b7bb5a4de38fe8682","permalink":"/news/neurips19/","publishdate":"2019-11-24T00:00:00Z","relpermalink":"/news/neurips19/","section":"news","summary":"This year's NeurIPS is here at Vancouver and I'm excited to share that I have been selected as volunteer.\nI'll be at the Registration Desk on Dec 9th, Monday morning, from 7am to 12pm, and also will be monitoring doors on Dec 11th, Wednesday afternoon, from 1:45pm to 5pm.\nPlease feel free to get in touch!","tags":null,"title":"I have been selected as student volunteer for NeurIPS 2019","type":"news"},{"authors":["Ricardo Carvalho"],"categories":["R"],"content":"Adaptive Lasso is an evolution of the Lasso. Let's see briefly how it improves Lasso and show the code needed to run it in R!\nLasso was introduced in this post, in case you don't know the method, please read about it here before!\n Oracle Procedure Before we enter the Adaptive Lasso it is important to know what is a procedure known as \u0026ldquo;Oracle\u0026rdquo;.\nAn oracle procedure is one that has the following oracle properties:\n Identifies the right subset of true variables; and Has optimal estimation rate.  Some studies (Zou 2006) state that the Lasso does not have the oracle properties. They claim that there are cases where a given $\\lambda$ that leads to optimal estimation rate ends up with inconsistent selection of variables (for example, includes noise variables). Similarly, there are also cases with the right selection of variables but showing biased estimates for large coefficients, leading to suboptimal prediction rates.\nTherefore, seeing that the Lasso is not an oracle procedure, Adaptive Lasso was developed to address this issue.\n Adaptive Lasso Adaptive Lasso is an evolution of the Lasso that has the oracle properties (for a suitable choice of $\\lambda$).\nAdaptive Lasso, as a regularization method, avoids overfitting penalizing large coefficients. Besides, it has the same advantage that Lasso: it can shrink some of the coefficients to exactly zero, performing thus a selection of attributes with the regularization.\nIn a linear regression, the Adaptive Lasso seeks to minimize:\n$$RSS(\\beta) + \\lambda \\sum_{j=1}^{p} \\hat{\\omega_j} |\\beta_j|$$\nwhere $\\lambda$ is the tuning parameter (chosen through 10-fold cross validation), $\\beta_j$ are the estimated coefficients, existing $p$ of them. Furthermore, we see $\\hat{\\omega_j}$, called Adaptive Weights vector, the edge of the Adaptive Lasso.\nWith $\\hat{\\omega_j}$ we are performing a different regularization for each coefficient, i.e., this vector adjusts the penalty differently for each coefficient. The Adaptive Weights vector is defined as:\n$$\\hat{\\omega_j} = \\frac{1}{\\left(|\\hat{\\beta_j}^{ini}|\\right)^{\\gamma}}$$\nIn the above equation $\\hat{\\beta_j}^{ini}$ is an initial estimate of the coefficients, usually obtained through Ridge Regression. So Adaptive Lasso ends up penalizing more those coefficients with lower initial estimates.\nWhereas $\\gamma$ is a positive constant for adjustment of the Adaptive Weights vector, and the authors suggest the possible values of 0.5, 1 and 2.\n Adaptive Lasso in R To run Adaptive Lasso in R, we will use the glmnet package, performing Ridge Regression to create the Adaptive Weights vector, as shown below.\nrequire(glmnet) ## Data = considering that we have a data frame named dataF, with its first column being the class x \u0026lt;- as.matrix(dataF[,-1]) # Removes class y \u0026lt;- as.double(as.matrix(dataF[, 1])) # Only class ## Ridge Regression to create the Adaptive Weights Vector set.seed(999) cv.ridge \u0026lt;- cv.glmnet(x, y, family='binomial', alpha=0, parallel=TRUE, standardize=TRUE) w3 \u0026lt;- 1/abs(matrix(coef(cv.ridge, s=cv.ridge$lambda.min) [, 1][2:(ncol(x)+1)] ))^1 ## Using gamma = 1 w3[w3[,1] == Inf] \u0026lt;- 999999999 ## Replacing values estimated as Infinite for 999999999 ## Adaptive Lasso set.seed(999) cv.lasso \u0026lt;- cv.glmnet(x, y, family='binomial', alpha=1, parallel=TRUE, standardize=TRUE, type.measure='auc', penalty.factor=w3) plot(cv.lasso) plot(cv.lasso$glmnet.fit, xvar=\u0026quot;lambda\u0026quot;, label=TRUE) abline(v = log(cv.lasso$lambda.min)) abline(v = log(cv.lasso$lambda.1se)) coef(cv.lasso, s=cv.lasso$lambda.1se) coef \u0026lt;- coef(cv.lasso, s='lambda.1se') selected_attributes \u0026lt;- (coef@i[-1]+1) ## Considering the structure of the data frame dataF as shown earlier  In the above code, we execute logistic regression (note the family='binomial\u0026rsquo;), in parallel (if a cluster or cores have been previously allocated), internally standardizing (needed for more appropriate regularization) and wanting to observe the results of AUC (area under ROC curve). Moreover, the method already performs 10-fold cross validation to choose the best $\\lambda$.\nFixing $\\gamma = 1$ (might be useful to vary it between the suggested values: 0.5, 1 and 2), we apply the Adaptive Weights vector on the cv.glmnet function using the argument penalty.factor.\nAt the end, there are some useful commands to verify the results, like plots of the AUC results and values of minimum $\\lambda$ (for minimum AUC) and 1 std. error (for AUC lower than minimum by one standard deviation), besides plot of the regularization perfomed.\n That's it! Adaptive Lasso can be very useful, so do not hesitate to test it!\nAny questions, suggestions: please comment!\n","date":1466121600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1466121600,"objectID":"1800c733bb2000fb4ca7d7d241a08b87","permalink":"/post/lasso/","publishdate":"2016-06-17T00:00:00Z","relpermalink":"/post/lasso/","section":"post","summary":"Adaptive Lasso is an evolution of the Lasso. Let's see briefly how it improves Lasso and show the code needed to run it in R!\nLasso was introduced in this post, in case you don't know the method, please read about it here before!\n Oracle Procedure Before we enter the Adaptive Lasso it is important to know what is a procedure known as \u0026ldquo;Oracle\u0026rdquo;.\nAn oracle procedure is one that has the following oracle properties:","tags":["Ridge","Lasso","Adaptive Lasso","R"],"title":"Adaptive Lasso: What it is and how to implement in R","type":"post"},{"authors":["Ricardo Carvalho"],"categories":["R"],"content":"In a very simple and direct way, after a brief introduction of the methods, we will see how to run Ridge Regression and Lasso using R!\n Ridge Regression in R Ridge Regression is a regularization method that tries to avoid overfitting, penalizing large coefficients through the L2 Norm. For this reason, it is also called L2 Regularization.\nIn a linear regression, in practice it means we are minimizing the RSS (Residual Sum of Squares) added to the L2 Norm. Thus, we seek to minimize:\n$$ RSS(\\beta) + \\lambda \\sum_{j=1}^{p} \\beta_j^2 $$\nwhere $\\lambda$ is the tuning parameter, $\\beta_j$ are the estimated coefficients, existing $p$ of them.\nTo perform Ridge Regression in R, we will use the glmnet package, developed by the creators of the algorithm.\nrequire(glmnet) # Data = considering that we have a data frame named dataF, with its first column being the class x \u0026lt;- as.matrix(dataF[,-1]) # Removes class y \u0026lt;- as.double(as.matrix(dataF[, 1])) # Only class # Fitting the model (Ridge: Alpha = 0) set.seed(999) cv.ridge \u0026lt;- cv.glmnet(x, y, family='binomial', alpha=0, parallel=TRUE, standardize=TRUE, type.measure='auc') # Results plot(cv.ridge) cv.ridge$lambda.min cv.ridge$lambda.1se coef(cv.ridge, s=cv.ridge$lambda.min)  In the above code, we execute logistic regression (note the family='binomial\u0026rsquo;), in parallel (if a cluster or cores have been previously allocated), internally standardizing (needed for more appropriate regularization) and wanting to observe the results of AUC (area under ROC curve). Moreover, the method already performs 10-fold cross validation to choose the best $\\lambda$.\nAt the end, there are some useful commands to verify the results, like plots of the AUC results and values of minimum $\\lambda$ (for minimum AUC) and 1 std. error (for AUC lower than minimum by one standard deviation).\n Lasso in R Now let's move to the Lasso! Lasso is also a regularization method that tries to avoid overfitting penalizing large coefficients, but it uses the L1 Norm. For this reason, it is also called L1 Regularization.\nThis method has as great advantage the fact that it can shrink some of the coefficients to exactly zero, performing thus a selection of attributes with the regularization.\nIn a linear regression, in practice for the Lasso, it means we are minimizing the RSS (Residual Sum of Squares) added to the L1 Norm. Thus, we seek to minimize:\n$RSS(\\beta) + \\lambda \\sum_{j=1}^{p} |\\beta_j|$\nwhere $\\lambda$ is the tuning parameter, $\\beta_j$ are the estimated coefficients, existing $p$ of them.\nTo perform Lasso in R, we will use the glmnet package, developed by the creators of the algorithm.\nrequire(glmnet) # Data = considering that we have a data frame named dataF, with its first column being the class x \u0026lt;- as.matrix(dataF[,-1]) # Removes class y \u0026lt;- as.double(as.matrix(dataF[, 1])) # Only class # Fitting the model (Lasso: Alpha = 1) set.seed(999) cv.lasso \u0026lt;- cv.glmnet(x, y, family='binomial', alpha=1, parallel=TRUE, standardize=TRUE, type.measure='auc') # Results plot(cv.lasso) plot(cv.lasso$glmnet.fit, xvar=\u0026quot;lambda\u0026quot;, label=TRUE) cv.lasso$lambda.min cv.lasso$lambda.1se coef(cv.lasso, s=cv.lasso$lambda.min)  In the above code, we execute logistic regression (note the family='binomial\u0026rsquo;), in parallel (if a cluster or cores have been previously allocated), internally standardizing (needed for more appropriate regularization) and wanting to observe the results of AUC (area under ROC curve). Moreover, the method already performs 10-fold cross validation to choose the best $\\lambda$.\nAt the end, there are some useful commands to verify the results, like plots of the AUC results and values of minimum $\\lambda$ (for minimum AUC) and 1 std. error (for AUC lower than minimum by one standard deviation).\n Since this post is already long, in another post I will talk about the Adaptive Lasso, an evolution of the Lasso seeking to satisfy the Oracle property.\nThat's it!\nAny questions, suggestions: feel free to comment!\n","date":1465862400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1465862400,"objectID":"c61ef0213ef15e4ef8db5e3da7772a4d","permalink":"/post/ridge/","publishdate":"2016-06-14T00:00:00Z","relpermalink":"/post/ridge/","section":"post","summary":"After a brief introduction of the methods, we will see how to run Ridge Regression and Lasso using R.","tags":["Ridge","Lasso","R"],"title":"How to use Ridge Regression and Lasso in R","type":"post"},{"authors":null,"categories":null,"content":"Please feel free to reach me using one of the options below.\n  Ricardo Carvalho   ricardosc [at] gmail [dot] com  rsilvaca [at] sfu [dot] ca  LinkedIn  Twitter   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3c4864f00d23f7ea35511ec930ce1d9c","permalink":"/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"Please feel free to reach me using one of the options below.\n  Ricardo Carvalho   ricardosc [at] gmail [dot] com  rsilvaca [at] sfu [dot] ca  LinkedIn  Twitter   ","tags":null,"title":"Contact","type":"page"}]