<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ricardo Carvalho</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Ricardo Carvalho</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 06 Jan 2020 18:58:22 -0300</lastBuildDate>
    <image>
      <url>/images/icon_huab70fe7c723e86c2f798430df322c2d8_42061_512x512_fill_lanczos_center_2.png</url>
      <title>Ricardo Carvalho</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Differentially Private Top-k Selection for Unknown Data Domain</title>
      <link>/publication/differentially-private-top-k-selection-for-unknown-data-domain/</link>
      <pubDate>Mon, 06 Jan 2020 18:58:22 -0300</pubDate>
      <guid>/publication/differentially-private-top-k-selection-for-unknown-data-domain/</guid>
      <description>&lt;p&gt;Currently, I&#39;m updating the latest version to address reviewersâ€™ comments and will soon release the full final paper.&lt;/p&gt;
&lt;p&gt;We also have extensive code to reproduce all the results, in pure python and also notebooks to simplify understanding. So stay tuned!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Paper accepted to AISTATS 2020 on differentially private top-k selection</title>
      <link>/news/aistats20/</link>
      <pubDate>Mon, 06 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/news/aistats20/</guid>
      <description>&lt;p&gt;After an entire year of doing research on Differential Privacy, I had my first paper as a Ph.D. student accepted to an important conference: AISTATS.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;u&gt;Title&lt;/u&gt;: Differentially Private Top-k Selection for Unknown Data Domain&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Authors&lt;/u&gt;: &lt;strong&gt;Ricardo Silva Carvalho&lt;/strong&gt;, Ke Wang, Lovedeep Gondara&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Conference&lt;/u&gt;: AISTATS 2020, the 23rd International Conference on Artificial Intelligence and Statistics&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I&#39;m now updating the latest version to address reviewers&amp;rsquo; comments and will soon release the full final paper.&lt;/p&gt;
&lt;p&gt;We also have extensive code to reproduce all the results, in python with notebooks to simplify understanding.&lt;/p&gt;
&lt;p&gt;Stay tuned!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Winning Privately: The Differentially Private Lottery Ticket Mechanism</title>
      <link>/publication/winning-privately-the-differentially-private-lottery-ticket-mechanism/</link>
      <pubDate>Sat, 14 Dec 2019 19:29:28 -0300</pubDate>
      <guid>/publication/winning-privately-the-differentially-private-lottery-ticket-mechanism/</guid>
      <description>&lt;p&gt;This is a work in progress presented at the Workshop on Machine Learning with Guarantees, part of NeurIPS 2019, as a &lt;strong&gt;POSTER&lt;/strong&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>I have been selected as student volunteer for NeurIPS 2019</title>
      <link>/news/neurips19/</link>
      <pubDate>Sun, 24 Nov 2019 00:00:00 +0000</pubDate>
      <guid>/news/neurips19/</guid>
      <description>&lt;p&gt;This year&#39;s NeurIPS is here at Vancouver and I&#39;m excited to share that I have been selected as volunteer.&lt;/p&gt;
&lt;p&gt;I&#39;ll be at the Registration Desk on Dec 9th, Monday morning, from 7am to 12pm, and also will be monitoring doors on Dec 11th, Wednesday afternoon, from 1:45pm to 5pm.&lt;/p&gt;
&lt;p&gt;Please feel free to get in touch!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>/privacy/</guid>
      <description>&lt;p&gt;Add your privacy policy here and set &lt;code&gt;draft: false&lt;/code&gt; to publish it. Otherwise, delete this file if you don&#39;t need it.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Terms</title>
      <link>/terms/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>/terms/</guid>
      <description>&lt;p&gt;Add your terms here and set &lt;code&gt;draft: false&lt;/code&gt; to publish it. Otherwise, delete this file if you don&#39;t need it.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Adaptive Lasso: What it is and how to implement in R</title>
      <link>/post/lasso/</link>
      <pubDate>Fri, 17 Jun 2016 00:00:00 +0000</pubDate>
      <guid>/post/lasso/</guid>
      <description>&lt;p&gt;Adaptive Lasso is an evolution of the Lasso. Let&#39;s see briefly how it improves Lasso and show the code needed to run it in R!&lt;/p&gt;
&lt;p&gt;Lasso was introduced &lt;a href=&#39;/post/ridge&#39;&gt;in this post&lt;/a&gt;, in case you don&#39;t know the method, please read about it &lt;a href=&#39;/post/ridge&#39;&gt;here&lt;/a&gt; before!&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;oracle-procedure&#34;&gt;Oracle Procedure&lt;/h2&gt;
&lt;p&gt;Before we enter the Adaptive Lasso it is important to know what is a procedure known as &amp;ldquo;Oracle&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;An oracle procedure is one that has the following oracle properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Identifies the right subset of true variables; and&lt;/li&gt;
&lt;li&gt;Has optimal estimation rate.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some studies &lt;a href=&#39;http://pages.cs.wisc.edu/~shao/stat992/zou2006.pdf&#39; target=&#39;_blank&#39;&gt;(Zou 2006)&lt;/a&gt; state that the Lasso does not have the oracle properties. They claim that there are cases where a given $\lambda$ that leads to optimal estimation rate ends up with inconsistent selection of variables (for example, includes noise variables). Similarly, there are also cases with the right selection of variables but showing biased estimates for large coefficients, leading to suboptimal prediction rates.&lt;/p&gt;
&lt;p&gt;Therefore, seeing that the Lasso is not an oracle procedure, Adaptive Lasso was developed to address this issue.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;adaptive-lasso&#34;&gt;Adaptive Lasso&lt;/h2&gt;
&lt;p&gt;Adaptive Lasso is an evolution of the Lasso that has the oracle properties (for a suitable choice of $\lambda$).&lt;/p&gt;
&lt;p&gt;Adaptive Lasso, as a regularization method, avoids overfitting penalizing large coefficients. Besides, it has the same advantage that Lasso: it can shrink some of the coefficients to exactly zero, performing thus a selection of attributes with the regularization.&lt;/p&gt;
&lt;p&gt;In a linear regression, the Adaptive Lasso seeks to minimize:&lt;/p&gt;
&lt;p&gt;$$RSS(\beta) + \lambda \sum_{j=1}^{p} \hat{\omega_j} |\beta_j|$$&lt;/p&gt;
&lt;p&gt;where $\lambda$ is the tuning parameter (chosen through 10-fold cross validation), $\beta_j$ are the estimated coefficients, existing $p$ of them. Furthermore, we see $\hat{\omega_j}$, called Adaptive Weights vector, the edge of the Adaptive Lasso.&lt;/p&gt;
&lt;p&gt;With $\hat{\omega_j}$ we are performing a different regularization for each coefficient, i.e., this vector adjusts the penalty differently for each coefficient. The Adaptive Weights vector is defined as:&lt;/p&gt;
&lt;p&gt;$$\hat{\omega_j} = \frac{1}{\left(|\hat{\beta_j}^{ini}|\right)^{\gamma}}$$&lt;/p&gt;
&lt;p&gt;In the above equation $\hat{\beta_j}^{ini}$ is an initial estimate of the coefficients, usually obtained through &lt;a href=&#39;/post/ridge&#39;&gt;Ridge Regression&lt;/a&gt;. So Adaptive Lasso ends up penalizing more those coefficients with lower initial estimates.&lt;/p&gt;
&lt;p&gt;Whereas $\gamma$ is a positive constant for adjustment of the Adaptive Weights vector, and the authors suggest the possible values of 0.5, 1 and 2.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;adaptive-lasso-in-r&#34;&gt;Adaptive Lasso in R&lt;/h2&gt;
&lt;p&gt;To run Adaptive Lasso in R, we will use the glmnet package, performing &lt;a href=&#39;/post/ridge&#39;&gt;Ridge Regression&lt;/a&gt; to create the Adaptive Weights vector, as shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;require(glmnet)
## Data = considering that we have a data frame named dataF, with its first column being the class
x &amp;lt;- as.matrix(dataF[,-1]) # Removes class
y &amp;lt;- as.double(as.matrix(dataF[, 1])) # Only class

## Ridge Regression to create the Adaptive Weights Vector
set.seed(999)
cv.ridge &amp;lt;- cv.glmnet(x, y, family=&#39;binomial&#39;, alpha=0, parallel=TRUE, standardize=TRUE)
w3 &amp;lt;- 1/abs(matrix(coef(cv.ridge, s=cv.ridge$lambda.min)
[, 1][2:(ncol(x)+1)] ))^1 ## Using gamma = 1
w3[w3[,1] == Inf] &amp;lt;- 999999999 ## Replacing values estimated as Infinite for 999999999

## Adaptive Lasso
set.seed(999)
cv.lasso &amp;lt;- cv.glmnet(x, y, family=&#39;binomial&#39;, alpha=1, parallel=TRUE, standardize=TRUE, type.measure=&#39;auc&#39;, penalty.factor=w3)
plot(cv.lasso)
plot(cv.lasso$glmnet.fit, xvar=&amp;quot;lambda&amp;quot;, label=TRUE)
abline(v = log(cv.lasso$lambda.min))
abline(v = log(cv.lasso$lambda.1se))
coef(cv.lasso, s=cv.lasso$lambda.1se)
coef &amp;lt;- coef(cv.lasso, s=&#39;lambda.1se&#39;)
selected_attributes &amp;lt;- (coef@i[-1]+1) ## Considering the structure of the data frame dataF as shown earlier
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above code, we execute logistic regression (note the family=&#39;binomial&amp;rsquo;), in parallel (if a cluster or cores have been previously allocated), internally standardizing (needed for more appropriate regularization) and wanting to observe the results of AUC (area under ROC curve). Moreover, the method already performs 10-fold cross validation to choose the best $\lambda$.&lt;/p&gt;
&lt;p&gt;Fixing $\gamma = 1$ (might be useful to vary it between the suggested values: 0.5, 1 and 2), we apply the Adaptive Weights vector on the cv.glmnet function using the argument &lt;strong&gt;penalty.factor&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;At the end, there are some useful commands to verify the results, like plots of the AUC results and values of minimum $\lambda$ (for minimum AUC) and 1 std. error (for AUC lower than minimum by one standard deviation), besides plot of the regularization perfomed.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;That&#39;s it! Adaptive Lasso can be very useful, so do not hesitate to test it!&lt;/p&gt;
&lt;p&gt;Any questions, suggestions: please comment!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to use Ridge Regression and Lasso in R</title>
      <link>/post/ridge/</link>
      <pubDate>Tue, 14 Jun 2016 00:00:00 +0000</pubDate>
      <guid>/post/ridge/</guid>
      <description>&lt;p&gt;In a very simple and direct way, after a brief introduction of the methods, we will see how to run Ridge Regression and Lasso using R!&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;ridge-regression-in-r&#34;&gt;Ridge Regression in R&lt;/h2&gt;
&lt;p&gt;Ridge Regression is a regularization method that tries to avoid overfitting, penalizing large coefficients through the L2 Norm. For this reason, it is also called L2 Regularization.&lt;/p&gt;
&lt;p&gt;In a linear regression, in practice it means we are minimizing the RSS (Residual Sum of Squares) added to the L2 Norm. Thus, we seek to minimize:&lt;/p&gt;
&lt;p&gt;$$ RSS(\beta) + \lambda \sum_{j=1}^{p} \beta_j^2 $$&lt;/p&gt;
&lt;p&gt;where $\lambda$ is the tuning parameter, $\beta_j$ are the estimated coefficients, existing $p$ of them.&lt;/p&gt;
&lt;p&gt;To perform Ridge Regression in R, we will use the glmnet package, developed by the creators of the algorithm.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;require(glmnet)
# Data = considering that we have a data frame named dataF, with its first column being the class
x &amp;lt;- as.matrix(dataF[,-1]) # Removes class
y &amp;lt;- as.double(as.matrix(dataF[, 1])) # Only class

# Fitting the model (Ridge: Alpha = 0)
set.seed(999)
cv.ridge &amp;lt;- cv.glmnet(x, y, family=&#39;binomial&#39;, alpha=0, parallel=TRUE, standardize=TRUE, type.measure=&#39;auc&#39;)

# Results
plot(cv.ridge)
cv.ridge$lambda.min
cv.ridge$lambda.1se
coef(cv.ridge, s=cv.ridge$lambda.min)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above code, we execute logistic regression (note the family=&#39;binomial&amp;rsquo;), in parallel (if a cluster or cores have been previously allocated), internally standardizing (needed for more appropriate regularization) and wanting to observe the results of AUC (area under ROC curve). Moreover, the method already performs 10-fold cross validation to choose the best $\lambda$.&lt;/p&gt;
&lt;p&gt;At the end, there are some useful commands to verify the results, like plots of the AUC results and values of minimum $\lambda$ (for minimum AUC) and 1 std. error (for AUC lower than minimum by one standard deviation).&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;lasso-in-r&#34;&gt;Lasso in R&lt;/h2&gt;
&lt;p&gt;Now let&#39;s move to the Lasso! Lasso is also a regularization method that tries to avoid overfitting penalizing large coefficients, but it uses the L1 Norm. For this reason, it is also called L1 Regularization.&lt;/p&gt;
&lt;p&gt;This method has as great advantage the fact that it can shrink some of the coefficients to exactly zero, performing thus a selection of attributes with the regularization.&lt;/p&gt;
&lt;p&gt;In a linear regression, in practice for the Lasso, it means we are minimizing the RSS (Residual Sum of Squares) added to the L1 Norm. Thus, we seek to minimize:&lt;/p&gt;
&lt;p&gt;$RSS(\beta) + \lambda \sum_{j=1}^{p} |\beta_j|$&lt;/p&gt;
&lt;p&gt;where $\lambda$ is the tuning parameter, $\beta_j$ are the estimated coefficients, existing $p$ of them.&lt;/p&gt;
&lt;p&gt;To perform Lasso in R, we will use the glmnet package, developed by the creators of the algorithm.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;require(glmnet)
# Data = considering that we have a data frame named dataF, with its first column being the class
x &amp;lt;- as.matrix(dataF[,-1]) # Removes class
y &amp;lt;- as.double(as.matrix(dataF[, 1])) # Only class

# Fitting the model (Lasso: Alpha = 1)
set.seed(999)
cv.lasso &amp;lt;- cv.glmnet(x, y, family=&#39;binomial&#39;, alpha=1, parallel=TRUE, standardize=TRUE, type.measure=&#39;auc&#39;)

# Results
plot(cv.lasso)
plot(cv.lasso$glmnet.fit, xvar=&amp;quot;lambda&amp;quot;, label=TRUE)
cv.lasso$lambda.min
cv.lasso$lambda.1se
coef(cv.lasso, s=cv.lasso$lambda.min)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above code, we execute logistic regression (note the family=&#39;binomial&amp;rsquo;), in parallel (if a cluster or cores have been previously allocated), internally standardizing (needed for more appropriate regularization) and wanting to observe the results of AUC (area under ROC curve). Moreover, the method already performs 10-fold cross validation to choose the best $\lambda$.&lt;/p&gt;
&lt;p&gt;At the end, there are some useful commands to verify the results, like plots of the AUC results and values of minimum $\lambda$ (for minimum AUC) and 1 std. error (for AUC lower than minimum by one standard deviation).&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Since this post is already long, in another post I will talk about the Adaptive Lasso, an evolution of the Lasso seeking to satisfy the Oracle property.&lt;/p&gt;
&lt;p&gt;That&#39;s it!&lt;/p&gt;
&lt;p&gt;Any questions, suggestions: feel free to comment!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Contact</title>
      <link>/contact/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/contact/</guid>
      <description>&lt;p&gt;Please feel free to reach me using one of the options below.&lt;/p&gt;
&lt;div class=&#34;container footer&#34;&gt;
  &lt;br/&gt;
  &lt;h4&gt;
  Ricardo Carvalho
  &lt;/h4&gt;
  &lt;i class=&#34;fa fa-envelope&#34; style=&#34;font-size:17px;&#34;&gt;&lt;/i&gt; &lt;a href=&#34;&#34;&gt;ricardosc [at] gmail [dot] com&lt;/a&gt;&lt;br/&gt;
  &lt;i class=&#34;fa fa-envelope&#34; style=&#34;font-size:17px;&#34;&gt;&lt;/i&gt; &lt;a href=&#34;&#34;&gt;rsilvaca [at] sfu [dot] ca&lt;/a&gt;&lt;br/&gt;
  &lt;i class=&#34;fab fa-linkedin&#34; style=&#34;font-size:20px;&#34;&gt;&lt;/i&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://www.linkedin.com/in/ricardoscr/&#34;&gt;LinkedIn&lt;/a&gt;&lt;br/&gt;
  &lt;i class=&#34;fab fa-twitter&#34; style=&#34;font-size:20px;&#34;&gt;&lt;/i&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://twitter.com/ricardocarv_lho&#34;&gt;Twitter&lt;/a&gt;
  &lt;br/&gt;&lt;br/&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
